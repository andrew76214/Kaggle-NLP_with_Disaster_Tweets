{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBWWuPfuI_zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e8901e2-e6b4-4e2d-d700-ac1752cd37d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       Our Deeds are the Reason of this #earthquake M...\n",
            "1                  Forest fire near La Ronge Sask. Canada\n",
            "2       All residents asked to 'shelter in place' are ...\n",
            "3       13,000 people receive #wildfires evacuation or...\n",
            "4       Just got sent this photo from Ruby #Alaska as ...\n",
            "                              ...                        \n",
            "7608    Two giant cranes holding a bridge collapse int...\n",
            "7609    @aria_ahrary @TheTawniest The out of control w...\n",
            "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
            "7611    Police investigating after an e-bike collided ...\n",
            "7612    The Latest: More Homes Razed by Northern Calif...\n",
            "Name: text, Length: 7613, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(train_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 停用詞\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def replace_some(Str):\n",
        "  Str = re.sub(r'http[s]?://\\S+', \"\", Str)\n",
        "  Str = re.sub(r'@\\S+', \"\", Str)\n",
        "  Str = re.sub(r\"[^a-zA-Z\\s]\",\"\",Str.lower())\n",
        "  Str = Str.split()\n",
        "  Str = [word for word in Str if word not in stop_words]\n",
        "  return ' '.join(Str) #隔著' '黏合在一起\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(replace_some)\n",
        "train_data['keyword'] = train_data['keyword'].fillna(\"\").apply(replace_some)\n",
        "test_data['text'] = test_data['text'].apply(replace_some)"
      ],
      "metadata": {
        "id": "YEZmAeBqwRi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "Index = []\n",
        "\n",
        "def turn_into_sequences(Str):\n",
        "  global max_len\n",
        "  global Index\n",
        "  if Str == \"\":\n",
        "      return [0]\n",
        "  Str = Str.split()\n",
        "  Str_list = []\n",
        "  for i in Str:\n",
        "    if i not in Index:\n",
        "      Index.append(i)\n",
        "      Str_list.append(Index.index(i)+1)\n",
        "    else:\n",
        "      Str_list.append(Index.index(i)+1)\n",
        "  if len(Str_list) > max_len:\n",
        "    max_len = len(Str_list)\n",
        "  return Str_list\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(turn_into_sequences)\n",
        "test_data['text'] = test_data['text'].apply(turn_into_sequences)"
      ],
      "metadata": {
        "id": "-kUUdd7Hzesq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padding(sequences):\n",
        "  global max_len\n",
        "  while max_len>len(sequences):\n",
        "    sequences.append(0)\n",
        "  return sequences\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(padding)\n",
        "train_data['keyword'] = train_data['keyword'].apply(turn_into_sequences)"
      ],
      "metadata": {
        "id": "A5tRD4K22KlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data['keyword'].head())\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4C4pGl869kZ",
        "outputId": "1ef4ff1b-1cdf-4cce-a44d-474129407f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [0]\n",
            "1    [0]\n",
            "2    [0]\n",
            "3    [0]\n",
            "4    [0]\n",
            "Name: keyword, dtype: object\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(train_data['text'])\n",
        "y = np.array(train_data['target'])\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5C-Mv4kNZKn",
        "outputId": "54cf05f2-9456-4c1b-df77-e4e0b72c4e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[list([1, 2, 3, 4, 5, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            " list([8, 9, 10, 11, 12, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            " list([15, 16, 17, 18, 19, 20, 21, 17, 18, 22, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            " ...\n",
            " list([8096, 8083, 8084, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            " list([174, 3591, 5211, 5197, 101, 766, 5212, 5211, 442, 5213, 811, 5214, 3956, 5215, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            " list([1243, 3875, 12068, 1571, 27, 2429, 6336, 680, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "TcczRFXSB7uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)  # 文本數據\n",
        "        self.y = torch.tensor(y, dtype=torch.float)  # 標籤\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# 分割訓練和測試集\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 創建數據集和數據加載器\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset = TextDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "I4YinLEUQuPm",
        "outputId": "7b86b842-a618-4dcc-e8dc-a4b0d367fb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-e3fd486a19a2>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 創建數據集和數據加載器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-100-e3fd486a19a2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 文本數據\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 標籤\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # 卷積層1，32個3x3的卷積核，ReLU 激活\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        # 卷積層2，64個3x3的卷積核，ReLU 激活\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        # 卷積層3，128個3x3的卷積核，ReLU 激活\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # 全連接層\n",
        "        self.fc1 = nn.Linear(23,23)  # 請根據輸入圖像的尺寸調整\n",
        "        self.fc2 = nn.Linear(23, 10)  # 10 個類別（對應 MNIST 的 10 個數字）\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 依次通過各層，並且加上 ReLU 激活函數\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # 展平操作\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "\n",
        "        # 通過全連接層\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "2K8gsvkiELhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}