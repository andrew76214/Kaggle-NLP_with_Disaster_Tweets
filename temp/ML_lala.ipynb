{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mqhXms2uZ3B",
        "outputId": "0dc314e1-004e-4433-c429-090ccf3c5e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       Our Deeds are the Reason of this #earthquake M...\n",
            "1                  Forest fire near La Ronge Sask. Canada\n",
            "2       All residents asked to 'shelter in place' are ...\n",
            "3       13,000 people receive #wildfires evacuation or...\n",
            "4       Just got sent this photo from Ruby #Alaska as ...\n",
            "                              ...                        \n",
            "7608    Two giant cranes holding a bridge collapse int...\n",
            "7609    @aria_ahrary @TheTawniest The out of control w...\n",
            "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
            "7611    Police investigating after an e-bike collided ...\n",
            "7612    The Latest: More Homes Razed by Northern Calif...\n",
            "Name: text, Length: 7613, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(train_data['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 停用詞\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def replace_some(Str):\n",
        "  Str = re.sub(r'http[s]?://\\S+', \"\", Str)\n",
        "  Str = re.sub(r'@\\S+', \"\", Str)\n",
        "  Str = re.sub(r\"[^a-zA-Z\\s]\",\"\",Str.lower())\n",
        "  Str = Str.split()\n",
        "  Str = [word for word in Str if word not in stop_words]\n",
        "  return ' '.join(Str) #隔著' '黏合在一起\n",
        "\n",
        "train_data['text'] = train_data['text'].apply(replace_some)\n",
        "test_data['text'] = test_data['text'].apply(replace_some)"
      ],
      "metadata": {
        "id": "CigXBzJFrMfY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['text'])  #建立詞典\n",
        "sequences = tokenizer.texts_to_sequences(train_data['text'])\n",
        "#依照詞典index將文本轉成一連串數字\n",
        "\n",
        "x = pad_sequences(sequences)  #把每個text的sequences長度變得一樣(增長)''',maxlen = len(tokenizer.word_index)+1,padding='post'''\n",
        "y = np.array(train_data['target'])"
      ],
      "metadata": {
        "id": "-1B7N6a3rIs0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim = len(tokenizer.word_index)+1,output_dim=50),  # 嵌入層\n",
        "    SimpleRNN(32, activation='tanh'),  # RNN 層\n",
        "    Dense(1, activation='sigmoid')  # 輸出層，二元分類\n",
        "])\n",
        "#嵌入層的input_dim是輸入資料是幾維／一單位(+1是因為詞典還包含所有詞的索引?)\n",
        "#tanh則是把輸出控制在1~-1\n",
        "#總之50跟32是chatgpt的建議(?)他說這兩個數字適合\n",
        "#輸出層是1是因為最後只輸出target，sigmoid是給二元分類的(0~1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#optimizer:優化器\n",
        "#loss:損失函數\n",
        "#metrics:評估指標。選準確率(預測對的筆數／所有筆數)\n",
        "\n",
        "model.fit(x, y, epochs=5, batch_size=32, validation_split=0.2)\n",
        "#epochs:訓練5趟\n",
        "#batch_size:一次取多少數據進行訓練\n",
        "#validation:多少數據用來驗證"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zabphBrFrB9d",
        "outputId": "3ed5bfda-34a9-46b1-8963-007e76873afd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.6220 - loss: 0.6528 - val_accuracy: 0.7571 - val_loss: 0.5048\n",
            "Epoch 2/5\n",
            "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.9011 - loss: 0.2713 - val_accuracy: 0.7255 - val_loss: 0.5632\n",
            "Epoch 3/5\n",
            "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9691 - loss: 0.1088 - val_accuracy: 0.7380 - val_loss: 0.5885\n",
            "Epoch 4/5\n",
            "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9754 - loss: 0.0796 - val_accuracy: 0.7538 - val_loss: 0.6214\n",
            "Epoch 5/5\n",
            "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.9747 - loss: 0.0669 - val_accuracy: 0.7584 - val_loss: 0.6521\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f03f39c7cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(test_data['text'])\n",
        "x_test = pad_sequences(sequences_test)\n",
        "y_test = (model.predict(x_test)>0.7).astype(\"int32\")\n",
        "#以0.7為界是目前結果最佳的"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uruLivA-rgkh",
        "outputId": "3b75bfa9-3afd-4d25-ace2-e3c4c128109b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'id':test_data['id'],\n",
        "    'target':y_test.flatten()\n",
        "})\n",
        "#flatten:轉換成1D陣列\n",
        "submission.to_csv('lala_submission.csv',index = False)"
      ],
      "metadata": {
        "id": "QkMsS3fNuyeT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"kaggle accuracy:0.75390\")\n",
        "print(\"\\nreport:\")\n",
        "\n",
        "report = pd.DataFrame({\n",
        "    'accuracy':[0.9747],'loss':[0.0669],'val_acurracy':[0.7584],'val_loss':[0.6521]\n",
        "})\n",
        "report.index = [\"Epoch 5/5\"]\n",
        "print(report)"
      ],
      "metadata": {
        "id": "J5sbJ3YtEUoq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252f8b0f-ced5-41c7-bb9f-ce08e570b8b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle accuracy:0.75390\n",
            "\n",
            "report:\n",
            "           accuracy    loss  val_acurracy  val_loss\n",
            "Epoch 5/5    0.9747  0.0669        0.7584    0.6521\n"
          ]
        }
      ]
    }
  ]
}